{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    \n",
    "    n = sum([tc.numel(p) for p in model.parameters() if p.requires_grad])/1e6\n",
    "    return str(n) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tc.randint(0, 10, (3, 32, 32), dtype = tc.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, dropout = 0.1, activation = 'relu', mask = None, device = 'cpu'):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model    = d_model\n",
    "        self.nhead      = nhead \n",
    "        self.dropout    = dropout \n",
    "        self.activation = activation\n",
    "        self.mask       = mask\n",
    "        self.device     = device\n",
    "        #get k, q, v from x by linear layers\n",
    "        self.fc0         = nn.Linear(d_model, d_model).to(self.device)\n",
    "        self.fc1         = nn.Linear(d_model, d_model).to(self.device)\n",
    "        self.fc2         = nn.Linear(d_model, d_model).to(self.device)\n",
    "        self.fc_cat      = nn.Linear(nhead * d_model, d_model).to(self.device)\n",
    "        \n",
    "        \n",
    "        pass \n",
    "    def forward(self, k, q, v):\n",
    "        k = self.fc0(k)\n",
    "        k = nn.Dropout(self.dropout)(k)\n",
    "        k = self.activation(k)\n",
    "        \n",
    "        q = self.fc1(q)\n",
    "        q = nn.Dropout(self.dropout)(q)\n",
    "        q = self.activation(q)\n",
    "        \n",
    "        v = self.fc2(v)\n",
    "        v = nn.Dropout(self.dropout)(v)\n",
    "        v = self.activation(v)\n",
    "        \n",
    "        p = self.scaled_dot_product_attention(k, q, v)\n",
    "        \n",
    "        if self.nhead > 1:\n",
    "            for i in range(1, self.nhead):\n",
    "                \n",
    "                k = self.fc0(k)\n",
    "                k = nn.Dropout(self.dropout)(k)\n",
    "                k = self.activation(k)\n",
    "                \n",
    "                q = self.fc1(q)\n",
    "                q = nn.Dropout(self.dropout)(q)\n",
    "                q = self.activation(q)\n",
    "                \n",
    "                v = self.fc2(v)\n",
    "                v = nn.Dropout(self.dropout)(v)\n",
    "                v = self.activation(v)\n",
    "                \n",
    "                p = tc.cat([p, self.scaled_dot_product_attention(k, q, v)], dim = 2)\n",
    "                \n",
    "        p = self.fc_cat(p)\n",
    "        \n",
    "        return p.to(self.device) \n",
    "    \n",
    "    def scaled_dot_product_attention(self, k, q, v):\n",
    "        \n",
    "        scores = tc.matmul(k, q.transpose(-2, -1)) / tc.sqrt(tc.tensor(self.d_model))\n",
    "        if self.mask is not None:\n",
    "            scores = scores.masked_fill(self.mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = tc.matmul(attention_weights, v)\n",
    "        return output.to(self.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout = 0.1, activation = 'relu', eps = 1e6, device = 'cpu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.layers for MultiHeadAttention part\n",
    "        self.eps         = eps\n",
    "        self.dropout     = dropout \n",
    "        self.activation  = activation\n",
    "        self.d_model     = d_model\n",
    "        self.nhead       = nhead\n",
    "        self.device      = device\n",
    "        \n",
    "        #activation function\n",
    "        if activation   == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leakyrelu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            print(f'{activation}  activation function is not valid. Please select one of the following options: 1)relu 2)leakyrelu 3)gelu 4)sigmoid 5)silu 6)selu 7)tanh')\n",
    "        #Multi Head Attention\n",
    "        self.multi_head_attention = MultiHeadAttention(self.d_model, self.nhead, self.dropout, self.activation, mask = None, device = self.device)\n",
    "        \n",
    "        #self.layers for Norm part\n",
    "        self.norm0 = nn.LayerNorm(d_model, eps = eps).to(self.device)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps = eps).to(self.device)\n",
    "        \n",
    "        #self.layers for FeedForward part\n",
    "        self.ff4 = nn.Linear(d_model, dim_feedforward).to(self.device)\n",
    "        self.ff  = nn.Linear(dim_feedforward,     d_model).to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #positional encoding\n",
    "        x = x + self.positional_encoding(x.shape[-2])\n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        #Multi Head Attention\n",
    "        y = self.multi_head_attention(x, x, x)\n",
    "        y = nn.Dropout(self.dropout)(y)\n",
    "        y = self.activation(y)\n",
    "        \n",
    "        #add and normalize\n",
    "        z = x + y \n",
    "        z = self.norm0(z)\n",
    "        \n",
    "        #Feed Forward\n",
    "        y = self.ff4(z)\n",
    "        y = nn.Dropout(self.dropout)(y)\n",
    "        y = self.activation(y)\n",
    "        \n",
    "        y = self.ff(y)\n",
    "        \n",
    "        #add and normalize\n",
    "        z = z + y\n",
    "        y = self.norm1(z)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def positional_encoding(self, l, n = 100):\n",
    "        \n",
    "        position = tc.zeros(l, self.d_model)\n",
    "        \n",
    "        for k in range(l):\n",
    "            for i in range(int(self.d_model/2)):\n",
    "                \n",
    "                position[k][2 * i]     = tc.sin(tc.tensor(k / (n) ** (2 * i / self.d_model)))\n",
    "                position[k][2 * i + 1] = tc.cos(tc.tensor(k / (n) ** (2 * i / self.d_model)))\n",
    "                    \n",
    "        return position.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tc.zeros(2, 4, 4)\n",
    "mask[1, 2:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.152384M'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(nn.TransformerEncoderLayer(512, 16, 2048, 0.1, 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.084544M'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(Encoder(512, 16, 2048, 0, 'relu',1e-6, 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(32, 8, 2048, 0.1, 'relu',1e-6, 'cpu')(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (activation): ReLU()\n",
       "  (multi_head_attention): MultiHeadAttention(\n",
       "    (activation): ReLU()\n",
       "    (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_cat): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm0): LayerNorm((512,), eps=1000000.0, elementwise_affine=True)\n",
       "  (norm1): LayerNorm((512,), eps=1000000.0, elementwise_affine=True)\n",
       "  (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(512, 4, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout = 0.1, activation = 'relu', eps = 1e6, mask = None, device = 'cpu'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mask        = mask\n",
    "        self.eps         = eps\n",
    "        self.dropout     = dropout \n",
    "        self.activation  = activation\n",
    "        self.d_model     = d_model\n",
    "        self.nhead       = nhead\n",
    "        self.device      = device\n",
    "        \n",
    "        #activation function\n",
    "        if activation   == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leakyrelu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            print(f'{activation}  activation function is not valid. Please select one of the following options: 1)relu 2)leakyrelu 3)gelu 4)sigmoid 5)silu 6)selu 7)tanh')\n",
    "        \n",
    "        #self.layers for MaskedMultiHeadAttention part\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(self.d_model, self.nhead, self.dropout, self.activation, mask = self.mask, device = self.device)\n",
    "        \n",
    "        #self.layers for MultiHeadAttention part\n",
    "        self.multi_head_attention = MultiHeadAttention(self.d_model, self.nhead, self.dropout, self.activation, mask = None, device = self.device)\n",
    "        \n",
    "        #self.layers for Norm part\n",
    "        self.norm0 = nn.LayerNorm(d_model, eps = eps, device = device).to(self.device)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps = eps, device = device).to(self.device)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps = eps, device = device).to(self.device)\n",
    "        \n",
    "        #self.layers for FeedForward part\n",
    "        self.ff4 = nn.Linear(d_model, dim_feedforward, device = device).to(self.device)\n",
    "        self.ff  = nn.Linear(dim_feedforward,     d_model, device = device).to(self.device)\n",
    "        \n",
    "        #last layer\n",
    "        self.fc  = nn.Linear(d_model, d_model, device = device).to(self.device)\n",
    "    \n",
    "    def forward(self, target, encoder_out):\n",
    "        #positional encoding\n",
    "        target = target + self.positional_encoding(target.shape[-2])\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        #Masked Multi Head Attention\n",
    "        y = self.masked_multi_head_attention(target, target, target)\n",
    "        y = nn.Dropout(self.dropout)(y)\n",
    "        y = self.activation(y)\n",
    "        \n",
    "        #add and normalize\n",
    "        z = target + y \n",
    "        z = self.norm0(z)\n",
    "        \n",
    "        #Multi Head Attention\n",
    "        y = self.multi_head_attention(encoder_out, encoder_out, z)\n",
    "        y = nn.Dropout(self.dropout)(y)\n",
    "        y = self.activation(y)\n",
    "        \n",
    "        #add and normalize \n",
    "        z = z + y \n",
    "        z = self.norm0(z)\n",
    "        \n",
    "        #Feed Forward\n",
    "        y = self.ff4(z)\n",
    "        y = nn.Dropout(self.dropout)(y)\n",
    "        y = self.activation(y)\n",
    "        \n",
    "        y = self.ff(y)\n",
    "        \n",
    "        #add and normalize\n",
    "        z = z + y\n",
    "        y = self.norm1(z)\n",
    "        \n",
    "        #last layer\n",
    "        y = self.fc(y)\n",
    "        y = nn.Softmax(dim = -1)(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def positional_encoding(self, l, n = 100):\n",
    "        \n",
    "        position = tc.zeros(l, self.d_model)\n",
    "        \n",
    "        for k in range(l):\n",
    "            for i in range(int(self.d_model/2)):\n",
    "                \n",
    "                position[k][2 * i]     = tc.sin(tc.tensor(k / (n) ** (2 * i / self.d_model)))\n",
    "                position[k][2 * i + 1] = tc.cos(tc.tensor(k / (n) ** (2 * i / self.d_model)))\n",
    "                    \n",
    "        return position.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.204032M'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(nn.TransformerDecoderLayer(512, 8, 2048, 0, 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.136704M'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(Decoder(512, 8, 2048, 0, 'relu', 1e-6, None, 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decoder(32, 4, 2048, 0.1, 'relu', 1e-6, None, 'cpu')(x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model , nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout = 0.1, activation = 'relu', eps = 1e6, mask = None,  device = 'cpu'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_encoder_layers = num_encoder_layers \n",
    "        self.num_decoder_layers = num_decoder_layers \n",
    "        self.dim_feedforward    = dim_feedforward \n",
    "        self.nhead              = nhead \n",
    "        self.d_model            = d_model \n",
    "        self.dropout            = dropout \n",
    "        self.activation         = activation \n",
    "        self.eps                = eps \n",
    "        self.mask               = mask \n",
    "        self.device             = device\n",
    "        #init encoders\n",
    "        self.encoder_list = []\n",
    "        for i in range(self.num_encoder_layers):\n",
    "            self.encoder_list.append(Encoder(self.d_model, self.nhead, self.dim_feedforward, self.dropout, self.activation, self.eps, device = self.device))\n",
    "        self.encoder_layer = nn.Sequential(*self.encoder_list)\n",
    "        #init decoders\n",
    "        self.decoder_list = []\n",
    "        for i in range(self.num_decoder_layers):\n",
    "            self.decoder_list.append(Decoder(self.d_model, self.nhead, self.dim_feedforward, self.dropout, self.activation, self.eps, self.mask, device = self.device))\n",
    "        self.decoder_layer = nn.Sequential(*self.decoder_list)\n",
    "        \n",
    "        pass\n",
    "    def forward(self, x, target):\n",
    "        #Encoder\n",
    "        if self.num_encoder_layers == 0:\n",
    "            y = x \n",
    "        else:\n",
    "            y = self.encoder(x)\n",
    "        \n",
    "        #Decoder\n",
    "        if self.num_decoder_layers == 0:\n",
    "            None \n",
    "        else:\n",
    "            y = self.decoder(y, target)\n",
    "        \n",
    "        return y\n",
    "    def encoder(self, x):\n",
    "        y = self.encoder_layer[0](x)\n",
    "        if self.num_encoder_layers > 1:\n",
    "            for i in range(1, self.num_encoder_layers):\n",
    "                y = self.encoder_layer[i](y) \n",
    "        return y\n",
    "    \n",
    "    def decoder(self, decoder_in, target):\n",
    "        z = self.decoder_layer[0](target, decoder_in)\n",
    "        if self.num_decoder_layers > 1:\n",
    "            for i in range(1, self.num_decoder_layers):\n",
    "                z = self.decoder_layer[i](z, decoder_in)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29.427712M'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(nn.Transformer(512, 8, 4, 4, 2048, 0, 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'52.496384M'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(Transformer(512, 8, 4, 4, 2048, 0, 'relu', 1e-6, None, 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformer(32, 8, 10, 0, 2048, 0, 'relu', 1e-6, None, 'cuda')(x.cuda(), x.cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_layer): Sequential(\n",
       "    (0): Encoder(\n",
       "      (activation): ReLU()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (activation): ReLU()\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layer): Sequential(\n",
       "    (0): Decoder(\n",
       "      (activation): ReLU()\n",
       "      (masked_multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (activation): ReLU()\n",
       "      (masked_multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (activation): ReLU()\n",
       "      (masked_multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): Decoder(\n",
       "      (activation): ReLU()\n",
       "      (masked_multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (activation): ReLU()\n",
       "        (fc0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_cat): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (ff4): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (ff): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformer(512, 8, 2, 4, 2048, 0, 'relu', 1e-6, None, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (dropout3): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Transformer(512, 8, 4, 4, 2048, 0, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91a66feb288e5a3ba784db9ef5ea18e02a46e15100cc28412487b6abfb332ddd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
